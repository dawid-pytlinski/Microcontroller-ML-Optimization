{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compare_models(tf_results, pt_results):\n",
    "    # Rozmiary modeli\n",
    "    tf_model_size = os.path.getsize(\"quantized_model.tflite\")\n",
    "    pt_model_size = os.path.getsize(\"pruned_model.pth\")\n",
    "    \n",
    "    print(f\"Original TensorFlow Quantized Model Size: {tf_model_size / 1024:.2f} KB\")\n",
    "    print(f\"Pruned PyTorch Model Size: {pt_model_size / 1024:.2f} KB\\n\")\n",
    "    \n",
    "    print(f\"TensorFlow - Accuracy: {tf_results['accuracy']:.4f}\")\n",
    "    print(f\"TensorFlow - Training Time: {tf_results['training_time']:.2f} s\")\n",
    "    print(f\"TensorFlow - Inference Time: {tf_results['inference_time']:.2f} s\")\n",
    "    \n",
    "    print(f\"PyTorch - Accuracy: {pt_results['accuracy']:.4f}\")\n",
    "    print(f\"PyTorch - Training Time: {pt_results['training_time']:.2f} s\")\n",
    "    print(f\"PyTorch - Inference Time: {pt_results['inference_time']:.2f} s\")\n",
    "    \n",
    "    comparison_data = {\n",
    "        \"Metric\": [\"Model Size (KB)\", \"Accuracy\", \"Training Time (s)\", \"Inference Time (s)\"],\n",
    "        \"TensorFlow\": [tf_model_size / 1024, tf_results[\"accuracy\"], tf_results[\"training_time\"], tf_results[\"inference_time\"]],\n",
    "        \"PyTorch\": [pt_model_size / 1024, pt_results[\"accuracy\"], pt_results[\"training_time\"], pt_results[\"inference_time\"]]\n",
    "    }\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nComparison Table:\")\n",
    "    print(df_comparison)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    metrics = [\"Model Size (KB)\", \"Accuracy\", \"Training Time (s)\", \"Inference Time (s)\"]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        ax.bar([\"TensorFlow\", \"PyTorch\"], [df_comparison.loc[i, \"TensorFlow\"], df_comparison.loc[i, \"PyTorch\"]], color=['blue', 'orange'])\n",
    "        ax.set_title(metric)\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(range(len(tf_results[\"predictions\"])), tf_results[\"predictions\"], label=\"TensorFlow Predictions\", alpha=0.5, color=\"blue\")\n",
    "    plt.scatter(range(len(pt_results[\"predictions\"])), pt_results[\"predictions\"], label=\"PyTorch Predictions\", alpha=0.5, color=\"orange\")\n",
    "    plt.title(\"Predictions Comparison\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Predicted Label\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "tf_results = {\n",
    "    \"accuracy\": 0.9559,\n",
    "    \"training_time\": 120.25,\n",
    "    \"inference_time\": 0.35,\n",
    "    \"predictions\": np.random.randint(0, 10, 100)  \n",
    "}\n",
    "\n",
    "pt_results = {\n",
    "    \"accuracy\": 0.9601,\n",
    "    \"training_time\": 140.50,\n",
    "    \"inference_time\": 0.50,\n",
    "    \"predictions\": np.random.randint(0, 10, 100)  \n",
    "}\n",
    "\n",
    "compare_models(tf_results, pt_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
